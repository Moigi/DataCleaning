{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1JO60e53w89U__WC_x73aE7ib7A9Ne6WM",
      "authorship_tag": "ABX9TyO4oa2ciwnGAM+SEeUQqNBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moigi/DataCleaning/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark\n",
        "!pip install pyspark[sql]\n",
        "!pandas API on Spark\n",
        "!pip install pyspark[pandas_on_spark] plotly  # to plot your data, you can install plotly together."
      ],
      "metadata": {
        "id": "8g6N_ioFY7IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".master(\"local[1]\") \\\n",
        ".appName(\"SparkByExamples.com\")\\\n",
        ".getOrCreate()\n",
        "\n",
        "filePath=\"/content/drive/MyDrive/BreadBasket_DMS.csv\"\n",
        "df = spark.read.options(header='true', inferSchema='true') \\\n",
        "          .csv(filePath)\n",
        "df.printSchema()\n",
        "df.show(4,truncate=False)\n"
      ],
      "metadata": {
        "id": "PyHt7b1dln88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "spark_app = SparkSession.builder.appName('_').getOrCreate()\n",
        "# import the Window Function\n",
        "from pyspark.sql.window import Window\n",
        "#import the rank from pyspark.sql.functions\n",
        "from pyspark.sql.functions import rank\n",
        "#partition the dataframe based on the values in Item column and\n",
        "#order the rows in each partition based on Time column\n",
        "partition = Window.partitionBy(\"Item\").orderBy('Time')\n",
        "#Now mention rank for each row in RANK column\n",
        "df1=df.withColumn(\"RANK\",rank().over(partition))\n",
        "df1.show(2)\n"
      ],
      "metadata": {
        "id": "NAdQ2b_Qt8in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.sql.functions import desc\n",
        " \n",
        "df2 = df1.withColumn(\"index\",monotonically_increasing_id())\n",
        "df2.orderBy(desc(\"index\")).drop(\"index\").show(2)"
      ],
      "metadata": {
        "id": "clUCv7RGBoFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "QzLaLQOjcg_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "  \n",
        "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
        "  \n",
        "df = spark.read.text(\"/content/drive/MyDrive/romeo-juliet-pg1777.txt\")\n",
        "  \n",
        "dfr=df.selectExpr(\"split(value, ' ') as Text_Data_In_Rows_Using_Text\")\n",
        "dfr.show(4,False)"
      ],
      "metadata": {
        "id": "E7VMwmkBcX-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
        "\n",
        "from pyspark.sql.functions import col, regexp_replace\n",
        "df = spark.read.text(\"/content/drive/MyDrive/romeo-juliet-pg1777.txt\")\n",
        "df = df.select(regexp_replace(col('value'), '\\n', '').alias(\"text\"))\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "3d3_IMCTg_F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
        "vector_df = tokenizer.transform(df).select(\"vector\")\n",
        "\n",
        "vector_df.printSchema()\n",
        "vector_df.show(10)"
      ],
      "metadata": {
        "id": "kwaRMPfJo8CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# Define a list of stop words or use default list\n",
        "remover = StopWordsRemover()\n",
        "stopwords = remover.getStopWords() \n",
        "\n",
        "# Display default list\n",
        "stopwords[:10]"
      ],
      "metadata": {
        "id": "YLVTz4WsqMQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify input/output columns\n",
        "remover.setInputCol(\"vector\")\n",
        "remover.setOutputCol(\"vector_no_stopw\")\n",
        "\n",
        "# Transform existing dataframe with the StopWordsRemover\n",
        "vector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\")\n",
        "\n",
        "# Display\n",
        "vector_no_stopw_df.printSchema()\n",
        "vector_no_stopw_df.show()"
      ],
      "metadata": {
        "id": "JWihp1c-qUBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stemmer library\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "# Instantiate stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Quick test of the stemming function\n",
        "tokens = [\"thanks\", \"its\", \"proverbially\", \"unexpected\", \"running\"]\n",
        "for t in tokens:\n",
        "  print(stemmer.stem(t))"
      ],
      "metadata": {
        "id": "LVCqXyO6qac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stemmer python function\n",
        "def stem(in_vec):\n",
        "    out_vec = []\n",
        "    for t in in_vec:\n",
        "        t_stem = stemmer.stem(t)\n",
        "        if len(t_stem) > 2:\n",
        "            out_vec.append(t_stem)       \n",
        "    return out_vec\n",
        "\n",
        "# Create user defined function for stemming with return type Array<String>\n",
        "from pyspark.sql.types import *\n",
        "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
        "\n",
        "# Create new df with vectors containing the stemmed tokens \n",
        "vector_stemmed_df = (\n",
        "    vector_no_stopw_df\n",
        "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
        "        .select(\"vector_stemmed\")\n",
        "  )\n",
        "\n",
        "# Rename df and column for clarity\n",
        "production_df = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"unigrams\"))\n",
        "\n",
        "# Display\n",
        "production_df.printSchema()\n",
        "production_df.show()"
      ],
      "metadata": {
        "id": "_uVCFdh2qhNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stemmer python function\n",
        "def stem(in_vec):\n",
        "    out_vec = []\n",
        "    for t in in_vec:\n",
        "        t_stem = stemmer.stem(t)\n",
        "        if len(t_stem) > 2:\n",
        "            out_vec.append(t_stem)       \n",
        "    return out_vec\n",
        "\n",
        "# Create user defined function for stemming with return type Array<String>\n",
        "from pyspark.sql.types import *\n",
        "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
        "\n",
        "# Create new df with vectors containing the stemmed tokens \n",
        "vector_stemmed_df = (\n",
        "    vector_no_stopw_df\n",
        "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
        "        .select(\"vector_stemmed\")\n",
        "  )\n",
        "\n",
        "# Rename df and column for clarity\n",
        "production_df = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"unigrams\"))\n",
        "\n",
        "# Display\n",
        "production_df.printSchema()\n",
        "production_df.show()"
      ],
      "metadata": {
        "id": "tSG2UDu9qCFL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}